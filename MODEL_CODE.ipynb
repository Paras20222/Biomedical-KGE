{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11805725,"sourceType":"datasetVersion","datasetId":7414184}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport json\nimport os\nfrom tqdm import tqdm\nimport random\nimport argparse\nimport sys\n\nclass KGDataset(Dataset):\n    def __init__(self, triple_file, entity_to_id, relation_to_id):\n        self.entity_to_id = entity_to_id\n        self.relation_to_id = relation_to_id\n        self.num_entities = len(entity_to_id)\n        self.triples = []\n        with open(triple_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                h, r, t = line.strip().split('\\t')\n                if h in entity_to_id and r in relation_to_id and t in entity_to_id:\n                    self.triples.append((entity_to_id[h], relation_to_id[r], entity_to_id[t]))\n        self.triples = np.array(self.triples, dtype=np.int64)\n        self.positive_set = set(map(tuple, self.triples))\n    \n    def __len__(self):\n        return len(self.triples)\n\n    def __getitem__(self, idx):\n        h, r, t = self.triples[idx]\n        # Create positive triple tensor\n        pos = torch.tensor([h, r, t], dtype=torch.long)\n        neg_triples = []\n        # Create negative triple by corrupting head\n        neg_h = random.randint(0, self.num_entities - 1)\n        while (neg_h, r, t) in self.positive_set:\n            neg_h = random.randint(0, self.num_entities - 1)\n        neg_triples.append([neg_h, r, t])\n        # Create negative triple by corrupting tail\n        neg_t = random.randint(0, self.num_entities - 1)\n        while (h, r, neg_t) in self.positive_set:\n            neg_t = random.randint(0, self.num_entities - 1)\n        neg_triples.append([h, r, neg_t])\n        # Combine all triples with corresponding labels\n        neg_triples = torch.tensor(neg_triples, dtype=torch.long)\n        labels = torch.tensor([1, -1, -1], dtype=torch.float)\n        all_triples = torch.cat([pos.unsqueeze(0), neg_triples], dim=0)\n        return all_triples, labels\n\ndef collate_fn(batch):\n    triples = torch.cat([item[0] for item in batch])\n    labels = torch.cat([item[1] for item in batch])\n    return triples, labels\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AdvancedConvBlock(nn.Module):\n    \"\"\"Handles asymmetric padding for even kernel sizes with proper dimension handling\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super().__init__()\n        self.kernel_size = kernel_size\n        \n        # Since input sequence has length 3, we need to be careful with kernel sizes\n        # For a sequence of length 3, max kernel size should be 3\n        self.effective_kernel = min(kernel_size, 3)\n        \n        # Calculate padding based on effective kernel size\n        self.padding = (self.effective_kernel - 1) // 2\n        \n        self.depthwise = nn.Conv1d(\n            in_channels, in_channels, self.effective_kernel,\n            padding=self.padding, groups=in_channels\n        )\n        self.pointwise = nn.Conv1d(in_channels, out_channels, 1)\n        \n        # Residual connection\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv1d(in_channels, out_channels, 1),\n                nn.BatchNorm1d(out_channels)\n            )\n            \n        self.bn = nn.BatchNorm1d(out_channels)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        residual = self.shortcut(x)\n        \n        # Apply depthwise convolution\n        out = F.gelu(self.depthwise(x))\n        out = self.pointwise(out)\n        \n        # Ensure output and residual have same shape\n        if out.shape != residual.shape:\n            # If shapes don't match, use adaptive pooling to match dimensions\n            if out.shape[-1] != residual.shape[-1]:\n                out = F.adaptive_max_pool1d(out, residual.shape[-1])\n        \n        out = self.bn(out)\n        out = self.dropout(out)\n        out += residual\n        return F.gelu(out)\n\nclass KGEModel(nn.Module):\n    def __init__(self, entity_embeddings, relation_embeddings, \n                 filter_sizes=(3, 4, 5), num_filters=128, dropout=0.3, \n                 l2_lambda=1e-5, num_conv_layers=3):\n        super().__init__()\n        self.embed_dim = entity_embeddings.shape[1]\n        self.filter_sizes = filter_sizes\n        self.num_filters = num_filters\n        self.l2_lambda = l2_lambda\n\n        # Embedding layers\n        self.entity_embed = nn.Embedding.from_pretrained(\n            torch.FloatTensor(entity_embeddings), freeze=False\n        )\n        self.rel_embed = nn.Embedding.from_pretrained(\n            torch.FloatTensor(relation_embeddings), freeze=False\n        )\n\n        # Convolutional branches with dimension preservation\n        self.conv_branches = nn.ModuleList()\n        for fs in filter_sizes:\n            branch = nn.Sequential()\n            in_channels = self.embed_dim\n            for layer_idx in range(num_conv_layers):\n                out_channels = num_filters if layer_idx == num_conv_layers-1 else num_filters//2\n                branch.add_module(\n                    f\"conv_{fs}_{layer_idx}\",\n                    AdvancedConvBlock(\n                        in_channels,\n                        out_channels,\n                        kernel_size=fs\n                    )\n                )\n                in_channels = out_channels\n            branch.add_module(f\"pool_{fs}\", nn.AdaptiveMaxPool1d(1))\n            self.conv_branches.append(branch)\n\n        # Final dense layers\n        self.fc = nn.Sequential(\n            nn.Linear(len(filter_sizes)*num_filters, 512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 1)\n        )\n    \n    def forward(self, x):\n        h, r, t = x[:, 0], x[:, 1], x[:, 2]\n        h_emb = self.entity_embed(h)\n        r_emb = self.rel_embed(r)\n        t_emb = self.entity_embed(t)\n        \n        # Correct input shape: [batch, embed_dim, 3]\n        triple_emb = torch.stack([h_emb, r_emb, t_emb], dim=1).permute(0, 2, 1)\n        \n        # Process through branches\n        branch_outputs = []\n        for branch in self.conv_branches:\n            branch_out = branch(triple_emb)\n            branch_outputs.append(branch_out.squeeze(-1))\n            \n        combined = torch.cat(branch_outputs, dim=1)\n        return self.fc(combined).squeeze()\n\n    def l2_regularization(self):\n        return self.l2_lambda * sum(p.norm(2)**2 for p in self.parameters())\n\n\ndef kged_loss(scores, labels, model):\n    loss = torch.log(1 + torch.exp(-labels * scores)).mean()\n    return loss + model.l2_regularization()\n\n\ndef train_model(model, train_loader, val_loader, optimizer, device, epochs=10, patience=3):\n    model.to(device)\n    best_val_acc = 0.0\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        \n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', mininterval=10)\n        for batch_idx, (triples, labels) in enumerate(pbar):\n            triples = triples.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            scores = model(triples)\n            loss = kged_loss(scores, labels, model)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            predicted = (scores >= 0).float() * 2 - 1\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            \n            pbar.set_postfix({\n                'Loss': f'{total_loss/(batch_idx+1):.4f}',\n                'Acc': f'{correct/total:.4f}'\n            })\n        \n        val_acc = evaluate(model, val_loader, device)\n        print(f'Validation - Accuracy: {val_acc:.4f}')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            torch.save(model.state_dict(), '/kaggle/working/best_model.pt')\n            print(f'Model saved to /kaggle/working/best_model.pt')\n        else:\n            patience_counter += 1\n            print(f'No improvement for {patience_counter} epochs')\n        \n        if patience_counter >= patience:\n            print(f'Early stopping after {epoch+1} epochs')\n            break\n\n\ndef evaluate(model, loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for triples, labels in tqdm(loader, desc='Evaluating'):\n            triples = triples.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            scores = model(triples)\n            predicted = (scores >= 0).float() * 2 - 1\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total\n    print(f'Evaluation - Accuracy: {accuracy:.4f}')\n    return accuracy\n\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    \n    base_path = '/kaggle/input/embeddings-triplet-dataset/'\n    \n    entity_emb = np.load(os.path.join(base_path, 'entity_embeddings_continued.npy'))\n    rel_emb = np.load(os.path.join(base_path, 'relation_embeddings_continued.npy'))\n    \n    with open(os.path.join(base_path, 'entity_to_id.json'), 'r') as f:\n        entity_to_id = json.load(f)\n    with open(os.path.join(base_path, 'relation_to_id.json'), 'r') as f:\n        relation_to_id = json.load(f)\n    \n    train_set = KGDataset(\n        os.path.join(base_path, 'train.tsv'),\n        entity_to_id,\n        relation_to_id\n    )\n    val_set = KGDataset(\n        os.path.join(base_path, 'val.tsv'),\n        entity_to_id,\n        relation_to_id\n    )\n    test_set = KGDataset(\n        os.path.join(base_path, 'test.tsv'),\n        entity_to_id,\n        relation_to_id\n    )\n    \n    batch_size = 512\n    \n    train_loader = DataLoader(\n        train_set, batch_size=batch_size, shuffle=True,\n        collate_fn=collate_fn, num_workers=2, pin_memory=True, persistent_workers=True\n    )\n    val_loader = DataLoader(\n        val_set, batch_size=batch_size, shuffle=False,\n        collate_fn=collate_fn, num_workers=2, pin_memory=True, persistent_workers=True\n    )\n    test_loader = DataLoader(\n        test_set, batch_size=batch_size, shuffle=False,\n        collate_fn=collate_fn, num_workers=2, pin_memory=True, persistent_workers=True\n    )\n    \n    model = KGEModel(\n        entity_embeddings=entity_emb,\n        relation_embeddings=rel_emb,\n        filter_sizes=(3, 4, 5),\n        num_filters=128,\n        dropout=0.3,\n        num_conv_layers=3,\n        l2_lambda=1e-5\n    )\n    \n    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n    train_model(model, train_loader, val_loader, optimizer, device, epochs=4, patience=1)\n    \n    print(\"Loading best model for testing...\")\n    model.load_state_dict(torch.load('/kaggle/working/best_model.pt', map_location=device))\n    print(\"Testing model...\")\n    evaluate(model, test_loader, device)\n    \n    # print(\"Computing MRR and Hits@k ...\")\n    # test_triples = test_set.triples.tolist()\n    # compute_ranking_metrics(model, test_triples, entity_to_id, device, batch_size=512)\n\nif __name__ == \"__main__\":\n    sys.argv = [sys.argv[0]]\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:32:31.547204Z","iopub.execute_input":"2025-05-14T07:32:31.548008Z","iopub.status.idle":"2025-05-14T07:45:38.537279Z","shell.execute_reply.started":"2025-05-14T07:32:31.547971Z","shell.execute_reply":"2025-05-14T07:45:38.536506Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/4: 100%|██████████| 4240/4240 [03:06<00:00, 22.76it/s, Loss=0.2505, Acc=0.9251]\nEvaluating: 100%|██████████| 531/531 [00:07<00:00, 73.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation - Accuracy: 0.8920\nValidation - Accuracy: 0.8920\nModel saved to /kaggle/working/best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/4: 100%|██████████| 4240/4240 [03:04<00:00, 23.03it/s, Loss=0.1989, Acc=0.9357]\nEvaluating: 100%|██████████| 531/531 [00:06<00:00, 82.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation - Accuracy: 0.8953\nValidation - Accuracy: 0.8953\nModel saved to /kaggle/working/best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/4: 100%|██████████| 4240/4240 [03:04<00:00, 23.01it/s, Loss=0.1865, Acc=0.9397]\nEvaluating: 100%|██████████| 531/531 [00:06<00:00, 87.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation - Accuracy: 0.8971\nValidation - Accuracy: 0.8971\nModel saved to /kaggle/working/best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/4: 100%|██████████| 4240/4240 [03:03<00:00, 23.08it/s, Loss=0.1805, Acc=0.9422]\nEvaluating: 100%|██████████| 531/531 [00:06<00:00, 87.16it/s]\n/tmp/ipykernel_31/3203272779.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/best_model.pt', map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Evaluation - Accuracy: 0.8981\nValidation - Accuracy: 0.8981\nModel saved to /kaggle/working/best_model.pt\nLoading best model for testing...\nTesting model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 531/531 [00:07<00:00, 75.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation - Accuracy: 0.8971\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def inspect_checkpoint(checkpoint_path, device):\n    \"\"\"Examine the architecture of the saved model\"\"\"\n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        \n        # Analyze keys to determine model architecture\n        filter_sizes = set()\n        num_filters = None\n        num_conv_layers = 0\n        \n        for key in checkpoint.keys():\n            if 'conv_branches' in key:\n                parts = key.split('.')\n                if len(parts) >= 3 and 'conv_' in parts[2]:\n                    filter_info = parts[2].split('_')\n                    if len(filter_info) >= 2 and filter_info[1].isdigit():\n                        filter_size = int(filter_info[1])\n                        filter_sizes.add(filter_size)\n                        \n                        if len(filter_info) >= 3 and filter_info[2].isdigit():\n                            layer_idx = int(filter_info[2]) + 1\n                            num_conv_layers = max(num_conv_layers, layer_idx)\n            \n            if 'pointwise.weight' in key:\n                shape = checkpoint[key].shape\n                if len(shape) > 0:\n                    num_filters = max(num_filters or 0, shape[0])\n        \n        # Try FC layer if we couldn't determine filters\n        if num_filters is None and 'fc.0.weight' in checkpoint:\n            fc_shape = checkpoint['fc.0.weight'].shape\n            if len(filter_sizes) > 0:\n                num_filters = fc_shape[1] // len(filter_sizes)\n                \n        return {\n            'filter_sizes': tuple(sorted(filter_sizes)) if filter_sizes else (3, 4, 5),\n            'num_filters': num_filters if num_filters else 128,\n            'num_conv_layers': num_conv_layers if num_conv_layers > 0 else 3\n        }\n        \n    except Exception as e:\n        print(f\"Error inspecting checkpoint: {e}\")\n        return {\n            'filter_sizes': (3, 4, 5),\n            'num_filters': 128,\n            'num_conv_layers': 3\n        }\ndef load_model_with_compatible_weights(model, checkpoint_path, device):\n    \"\"\"Load a model checkpoint, handling parameter mismatches\"\"\"\n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        \n        # Get the model's state dict\n        model_dict = model.state_dict()\n        \n        # Create a filtered state dict with only compatible parameters\n        filtered_dict = {}\n        skipped_params = []\n        \n        for k, v in checkpoint.items():\n            if k in model_dict:\n                if v.shape == model_dict[k].shape:\n                    filtered_dict[k] = v\n                else:\n                    skipped_params.append((k, v.shape, model_dict[k].shape))\n        \n        # Load the filtered state dict\n        model.load_state_dict(filtered_dict, strict=False)\n        \n        print(f\"Successfully loaded {len(filtered_dict)}/{len(model_dict)} parameters\")\n        print(f\"Skipped {len(skipped_params)} parameters due to mismatch\")\n        \n        return model\n    \n    except Exception as e:\n        print(f\"Error loading checkpoint: {e}\")\n        return model\ndef compute_ranking_metrics(model, test_triples, all_triples, entity_to_id, device, sample_size=4000):\n    model.eval()\n    num_entities = len(entity_to_id)\n    entity_ids = torch.arange(num_entities).to(device)\n    \n    # Sample test triples if needed\n    test_triples = test_triples[:sample_size]\n    print(f\"Loaded {len(test_triples)} test triples for evaluation\")\n    \n    # Create filtered dictionary for evaluation\n    filter_dict = {}\n    for h, r, t in all_triples:\n        filter_dict.setdefault((r, t), set()).add(h)\n        filter_dict.setdefault((h, r), set()).add(t)\n    \n    ranks = []\n    hits = {1: 0, 3: 0, 10: 0}\n    \n    # Process in batches to avoid OOM\n    batch_size = 128\n    \n    with torch.no_grad():\n        for i, (h, r, t) in enumerate(tqdm(test_triples, desc=\"Evaluation\")):\n            # Head prediction\n            head_ranks = []\n            \n            for start in range(0, num_entities, batch_size):\n                end = min(start + batch_size, num_entities)\n                current_entities = entity_ids[start:end]\n                \n                # Create batch for head prediction\n                hr_batch = torch.zeros((len(current_entities), 3), dtype=torch.long, device=device)\n                hr_batch[:, 0] = current_entities\n                hr_batch[:, 1] = r\n                hr_batch[:, 2] = t\n                \n                # Get scores\n                scores = model(hr_batch).cpu().numpy()\n                \n                # Filter out other true triples\n                for j, e in enumerate(current_entities.cpu().numpy()):\n                    if e != h and (r, t) in filter_dict and e in filter_dict[(r, t)]:\n                        scores[j] = -np.inf\n                \n                # If true head is in this batch\n                if start <= h < end:\n                    h_idx = h - start\n                    h_score = scores[h_idx]\n                    h_rank = 1 + np.sum(scores > h_score)\n                    head_ranks.append(h_rank)\n            \n            if head_ranks:\n                ranks.append(min(head_ranks))\n            \n            # Tail prediction\n            tail_ranks = []\n            \n            for start in range(0, num_entities, batch_size):\n                end = min(start + batch_size, num_entities)\n                current_entities = entity_ids[start:end]\n                \n                # Create batch for tail prediction\n                tr_batch = torch.zeros((len(current_entities), 3), dtype=torch.long, device=device)\n                tr_batch[:, 0] = h\n                tr_batch[:, 1] = r\n                tr_batch[:, 2] = current_entities\n                \n                # Get scores\n                scores = model(tr_batch).cpu().numpy()\n                \n                # Filter out other true triples\n                for j, e in enumerate(current_entities.cpu().numpy()):\n                    if e != t and (h, r) in filter_dict and e in filter_dict[(h, r)]:\n                        scores[j] = -np.inf\n                \n                # If true tail is in this batch\n                if start <= t < end:\n                    t_idx = t - start\n                    t_score = scores[t_idx]\n                    t_rank = 1 + np.sum(scores > t_score)\n                    tail_ranks.append(t_rank)\n            \n            if tail_ranks:\n                ranks.append(min(tail_ranks))\n    \n    # Calculate metrics\n    ranks_array = np.array(ranks)\n    mrr = np.mean(1.0 / ranks_array)\n    \n    hits_metrics = {}\n    for k in [1, 3, 10]:\n        hits_metrics[k] = np.mean(ranks_array <= k)\n    \n    return {'MRR': mrr, **{f'Hits@{k}': hits_metrics[k] for k in hits_metrics}}\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    \n    # Load data\n    base_path = '/kaggle/input/embeddings-triplet-dataset/'\n    entity_emb = np.load(os.path.join(base_path, 'entity_embeddings_continued.npy'))\n    rel_emb = np.load(os.path.join(base_path, 'relation_embeddings_continued.npy'))\n    \n    with open(os.path.join(base_path, 'entity_to_id.json'), 'r') as f:\n        entity_to_id = json.load(f)\n    with open(os.path.join(base_path, 'relation_to_id.json'), 'r') as f:\n        relation_to_id = json.load(f)\n    \n    # First, inspect checkpoint to get architecture\n    checkpoint_path = '/kaggle/working/best_model.pt'\n    architecture = inspect_checkpoint(checkpoint_path, device)\n    \n    # Initialize model with detected architecture\n    model = KGEModel(\n        entity_embeddings=entity_emb,\n        relation_embeddings=rel_emb,\n        filter_sizes=architecture['filter_sizes'],\n        num_filters=architecture['num_filters'],\n        dropout=0.3,\n        num_conv_layers=architecture['num_conv_layers'],\n        l2_lambda=1e-5\n    ).to(device)\n    \n    # Load weights with compatibility handling\n    model = load_model_with_compatible_weights(model, checkpoint_path, device)\n    \n    # Load test data\n    test_triples = []\n    with open(os.path.join(base_path, 'test.tsv'), 'r') as f:\n        for line in f:\n            h, r, t = line.strip().split('\\t')\n            if h in entity_to_id and r in relation_to_id and t in entity_to_id:\n                test_triples.append((\n                    entity_to_id[h],\n                    relation_to_id[r],\n                    entity_to_id[t]\n                ))\n    \n    # Load all triples for filtering\n    all_triples = set()\n    for split in ['train.tsv', 'val.tsv', 'test.tsv']:\n        with open(os.path.join(base_path, split), 'r') as f:\n            for line in f:\n                h, r, t = line.strip().split('\\t')\n                if h in entity_to_id and r in relation_to_id and t in entity_to_id:\n                    all_triples.add((\n                        entity_to_id[h],\n                        relation_to_id[r],\n                        entity_to_id[t]\n                    ))\n    \n    print(f\"Loaded {len(all_triples)} total triples for filtering\")\n    \n    # Run evaluation\n    metrics = compute_ranking_metrics(\n        model, test_triples, all_triples, \n        entity_to_id, device, sample_size=200\n    )\n    \n    print(\"\\n=== Final Test Metrics ===\")\n    print(f\"MRR: {metrics['MRR']:.4f}\")\n    print(f\"Hits@1: {metrics['Hits@1']:.4f}\")\n    print(f\"Hits@3: {metrics['Hits@3']:.4f}\")\n    print(f\"Hits@10: {metrics['Hits@10']:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:54:40.366620Z","iopub.execute_input":"2025-05-14T07:54:40.367370Z","iopub.status.idle":"2025-05-14T08:07:15.618899Z","shell.execute_reply.started":"2025-05-14T07:54:40.367330Z","shell.execute_reply":"2025-05-14T08:07:15.618247Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nSuccessfully loaded 133/133 parameters\nSkipped 0 parameters due to mismatch\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2996557424.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n/tmp/ipykernel_31/2996557424.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded 1957551 total triples for filtering\nLoaded 200 test triples for evaluation\n","output_type":"stream"},{"name":"stderr","text":"Evaluation: 100%|██████████| 200/200 [12:29<00:00,  3.75s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n=== Final Test Metrics ===\nMRR: 0.3793\nHits@1: 0.2400\nHits@3: 0.4250\nHits@10: 0.6825\n","output_type":"stream"}],"execution_count":4}]}